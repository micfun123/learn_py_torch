{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self,csv_file,transform=None,is_test=False):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)  #return the total number of samples in the dataset.\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_frame.iloc[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            image = item.values.reshape(28,28).astype(np.uint8)\n",
    "            label = None\n",
    "        else:\n",
    "            image = item[1:].values.reshape(28,28).astype(np.uint8)\n",
    "            label = item.iloc[0]\n",
    "\n",
    "        image = transforms.ToPILImage()(image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.RandomRotation(15),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5),)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomMNISTDataset(csv_file='train.csv',transform=transform,is_test=False)\n",
    "test_dataset = CustomMNISTDataset(csv_file='test.csv',transform=transform,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 42003 Test Size: 28000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Size: {len(train_dataset)} Test Size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000,  0.0745,  0.5059, -0.3255,\n",
       "           -0.4353,  0.1843, -0.7490, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.8588, -0.7647,  0.0745,  0.9922,  0.9922,  0.7020,\n",
       "            0.9294,  0.1843,  0.8118, -0.5765, -0.8824, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8980,\n",
       "           -0.3255,  0.9608,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1843, -0.3333,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4039,\n",
       "            0.4039,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059, -0.3333,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4353,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1451, -0.3490,\n",
       "            0.5608,  0.9922,  0.5686,  0.9922,  0.9922,  0.9922,  0.3412,\n",
       "           -0.4039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4980,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.1529, -0.6471, -1.0000,\n",
       "           -0.9137, -0.7725,  0.0039,  0.9765,  0.9922,  0.9922,  0.6627,\n",
       "            0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5216,  0.9922,\n",
       "            0.9922,  0.9922,  0.5843,  0.1529, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.3490,  0.9922,  0.9922,  0.9922,\n",
       "            0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3490,  0.9922,\n",
       "            0.9922, -0.3020, -0.4745, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.8824,  0.9922,\n",
       "            0.9922,  0.7569, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -0.9922,  0.3647,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3020,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6314,  0.9922,\n",
       "            0.9922,  0.8824, -0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4118,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.9922,\n",
       "            0.9922,  0.4588, -0.9451, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4980,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n",
       "            0.9922,  0.1765,  0.1765, -0.9922, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8902,  0.8196,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,  0.6392,\n",
       "            0.9922,  0.9216, -0.7569, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8353,  0.2627,  0.9922,\n",
       "            0.9922,  0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,  0.2784,\n",
       "            0.9922,  0.9922,  0.9922,  0.5686, -0.9059, -0.4824, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.6235,  0.6392,  0.9922,  0.9922,\n",
       "            0.9922, -0.3255, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.2627,\n",
       "            0.6157,  0.9922,  0.9922,  0.9922,  0.5843,  0.5216, -0.6235,\n",
       "           -0.6235, -0.7333, -0.6784,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922, -0.9137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8824,\n",
       "           -0.5294,  0.6627,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.8275,  0.9059,  0.9922,  0.9922,  0.9922,  0.8745,\n",
       "            0.8745, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.5294, -0.3255,  0.9059,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059,\n",
       "           -0.4510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.1059,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  1.0000,  0.1451, -0.8510, -0.8824,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.8980,  0.4275,  0.9922,  0.1451,\n",
       "            0.9922,  1.0000,  0.9922, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.9373, -0.4039, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # num_workers=2 slows down the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for example_data, example_label in train_loader:\n",
    "    example_image = example_data[0]\n",
    "    print(f\"input size: {example_image.size()}\")\n",
    "    example_image_numpy = example_image.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(example_image_numpy)\n",
    "    plt.title(f\"Label: {example_label[0]}\")\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.cov1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.cov2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.cov3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        # Update the input size for fc1\n",
    "        self.fc1 = nn.Linear(128 * 26 * 26, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.cov1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.cov2(x)\n",
    "        x = self.cov3(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)   # Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, Loss: 1.9346\n",
      "Epoch: 1, Batch: 200, Loss: 0.6281\n",
      "Epoch: 1, Batch: 300, Loss: 0.4043\n",
      "Epoch: 1, Batch: 400, Loss: 0.3683\n",
      "Epoch: 2, Batch: 100, Loss: 0.3530\n",
      "Epoch: 2, Batch: 200, Loss: 0.2734\n",
      "Epoch: 2, Batch: 300, Loss: 0.2505\n",
      "Epoch: 2, Batch: 400, Loss: 0.2339\n",
      "Epoch: 3, Batch: 100, Loss: 0.2702\n",
      "Epoch: 3, Batch: 200, Loss: 0.1815\n",
      "Epoch: 3, Batch: 300, Loss: 0.1751\n",
      "Epoch: 3, Batch: 400, Loss: 0.1739\n",
      "Epoch: 4, Batch: 100, Loss: 0.1903\n",
      "Epoch: 4, Batch: 200, Loss: 0.1479\n",
      "Epoch: 4, Batch: 300, Loss: 0.1385\n",
      "Epoch: 4, Batch: 400, Loss: 0.1479\n",
      "Epoch: 5, Batch: 100, Loss: 0.2031\n",
      "Epoch: 5, Batch: 200, Loss: 0.1450\n",
      "Epoch: 5, Batch: 300, Loss: 0.1265\n",
      "Epoch: 5, Batch: 400, Loss: 0.1114\n",
      "Epoch: 6, Batch: 100, Loss: 0.1430\n",
      "Epoch: 6, Batch: 200, Loss: 0.1091\n",
      "Epoch: 6, Batch: 300, Loss: 0.1059\n",
      "Epoch: 6, Batch: 400, Loss: 0.1017\n",
      "Epoch: 7, Batch: 100, Loss: 0.1256\n",
      "Epoch: 7, Batch: 200, Loss: 0.0941\n",
      "Epoch: 7, Batch: 300, Loss: 0.0999\n",
      "Epoch: 7, Batch: 400, Loss: 0.0904\n",
      "Epoch: 8, Batch: 100, Loss: 0.1113\n",
      "Epoch: 8, Batch: 200, Loss: 0.0801\n",
      "Epoch: 8, Batch: 300, Loss: 0.0832\n",
      "Epoch: 8, Batch: 400, Loss: 0.0746\n",
      "Epoch: 9, Batch: 100, Loss: 0.0945\n",
      "Epoch: 9, Batch: 200, Loss: 0.0710\n",
      "Epoch: 9, Batch: 300, Loss: 0.0756\n",
      "Epoch: 9, Batch: 400, Loss: 0.0699\n",
      "Epoch: 10, Batch: 100, Loss: 0.0815\n",
      "Epoch: 10, Batch: 200, Loss: 0.0672\n",
      "Epoch: 10, Batch: 300, Loss: 0.0634\n",
      "Epoch: 10, Batch: 400, Loss: 0.0709\n",
      "Epoch: 11, Batch: 100, Loss: 0.0795\n",
      "Epoch: 11, Batch: 200, Loss: 0.0601\n",
      "Epoch: 11, Batch: 300, Loss: 0.0618\n",
      "Epoch: 11, Batch: 400, Loss: 0.0630\n",
      "Epoch: 12, Batch: 100, Loss: 0.0679\n",
      "Epoch: 12, Batch: 200, Loss: 0.0581\n",
      "Epoch: 12, Batch: 300, Loss: 0.0543\n",
      "Epoch: 12, Batch: 400, Loss: 0.0524\n",
      "Epoch: 13, Batch: 100, Loss: 0.0663\n",
      "Epoch: 13, Batch: 200, Loss: 0.0457\n",
      "Epoch: 13, Batch: 300, Loss: 0.0486\n",
      "Epoch: 13, Batch: 400, Loss: 0.0546\n",
      "Epoch: 14, Batch: 100, Loss: 0.0605\n",
      "Epoch: 14, Batch: 200, Loss: 0.0450\n",
      "Epoch: 14, Batch: 300, Loss: 0.0465\n",
      "Epoch: 14, Batch: 400, Loss: 0.0493\n",
      "Epoch: 15, Batch: 100, Loss: 0.0553\n",
      "Epoch: 15, Batch: 200, Loss: 0.0432\n",
      "Epoch: 15, Batch: 300, Loss: 0.0471\n",
      "Epoch: 15, Batch: 400, Loss: 0.0388\n",
      "Epoch: 16, Batch: 100, Loss: 0.0538\n",
      "Epoch: 16, Batch: 200, Loss: 0.0420\n",
      "Epoch: 16, Batch: 300, Loss: 0.0413\n",
      "Epoch: 16, Batch: 400, Loss: 0.0398\n",
      "Epoch: 17, Batch: 100, Loss: 0.0481\n",
      "Epoch: 17, Batch: 200, Loss: 0.0378\n",
      "Epoch: 17, Batch: 300, Loss: 0.0399\n",
      "Epoch: 17, Batch: 400, Loss: 0.0349\n",
      "Epoch: 18, Batch: 100, Loss: 0.0424\n",
      "Epoch: 18, Batch: 200, Loss: 0.0396\n",
      "Epoch: 18, Batch: 300, Loss: 0.0366\n",
      "Epoch: 18, Batch: 400, Loss: 0.0386\n",
      "Epoch: 19, Batch: 100, Loss: 0.0439\n",
      "Epoch: 19, Batch: 200, Loss: 0.0284\n",
      "Epoch: 19, Batch: 300, Loss: 0.0331\n",
      "Epoch: 19, Batch: 400, Loss: 0.0316\n",
      "Epoch: 20, Batch: 100, Loss: 0.0342\n",
      "Epoch: 20, Batch: 200, Loss: 0.0327\n",
      "Epoch: 20, Batch: 300, Loss: 0.0323\n",
      "Epoch: 20, Batch: 400, Loss: 0.0338\n",
      "Epoch: 21, Batch: 100, Loss: 0.0396\n",
      "Epoch: 21, Batch: 200, Loss: 0.0291\n",
      "Epoch: 21, Batch: 300, Loss: 0.0298\n",
      "Epoch: 21, Batch: 400, Loss: 0.0343\n",
      "Epoch: 22, Batch: 100, Loss: 0.0532\n",
      "Epoch: 22, Batch: 200, Loss: 0.0321\n",
      "Epoch: 22, Batch: 300, Loss: 0.0285\n",
      "Epoch: 22, Batch: 400, Loss: 0.0355\n",
      "Epoch: 23, Batch: 100, Loss: 0.0345\n",
      "Epoch: 23, Batch: 200, Loss: 0.0259\n",
      "Epoch: 23, Batch: 300, Loss: 0.0275\n",
      "Epoch: 23, Batch: 400, Loss: 0.0286\n",
      "Epoch: 24, Batch: 100, Loss: 0.0294\n",
      "Epoch: 24, Batch: 200, Loss: 0.0252\n",
      "Epoch: 24, Batch: 300, Loss: 0.0309\n",
      "Epoch: 24, Batch: 400, Loss: 0.0308\n",
      "Epoch: 25, Batch: 100, Loss: 0.0886\n",
      "Epoch: 25, Batch: 200, Loss: 0.0328\n",
      "Epoch: 25, Batch: 300, Loss: 0.0291\n",
      "Epoch: 25, Batch: 400, Loss: 0.0291\n",
      "Epoch: 26, Batch: 100, Loss: 0.0378\n",
      "Epoch: 26, Batch: 200, Loss: 0.0217\n",
      "Epoch: 26, Batch: 300, Loss: 0.0250\n",
      "Epoch: 26, Batch: 400, Loss: 0.0289\n",
      "Epoch: 27, Batch: 100, Loss: 0.0287\n",
      "Epoch: 27, Batch: 200, Loss: 0.0240\n",
      "Epoch: 27, Batch: 300, Loss: 0.0245\n",
      "Epoch: 27, Batch: 400, Loss: 0.0260\n",
      "Epoch: 28, Batch: 100, Loss: 0.0245\n",
      "Epoch: 28, Batch: 200, Loss: 0.0248\n",
      "Epoch: 28, Batch: 300, Loss: 0.0246\n",
      "Epoch: 28, Batch: 400, Loss: 0.0245\n",
      "Epoch: 29, Batch: 100, Loss: 0.0275\n",
      "Epoch: 29, Batch: 200, Loss: 0.0207\n",
      "Epoch: 29, Batch: 300, Loss: 0.0225\n",
      "Epoch: 29, Batch: 400, Loss: 0.0208\n",
      "Epoch: 30, Batch: 100, Loss: 0.0281\n",
      "Epoch: 30, Batch: 200, Loss: 0.0183\n",
      "Epoch: 30, Batch: 300, Loss: 0.0210\n",
      "Epoch: 30, Batch: 400, Loss: 0.0189\n",
      "Epoch: 31, Batch: 100, Loss: 0.0205\n",
      "Epoch: 31, Batch: 200, Loss: 0.0193\n",
      "Epoch: 31, Batch: 300, Loss: 0.0200\n",
      "Epoch: 31, Batch: 400, Loss: 0.0184\n",
      "Epoch: 32, Batch: 100, Loss: 0.0175\n",
      "Epoch: 32, Batch: 200, Loss: 0.0197\n",
      "Epoch: 32, Batch: 300, Loss: 0.0181\n",
      "Epoch: 32, Batch: 400, Loss: 0.0189\n",
      "Epoch: 33, Batch: 100, Loss: 0.0217\n",
      "Epoch: 33, Batch: 200, Loss: 0.0171\n",
      "Epoch: 33, Batch: 300, Loss: 0.0191\n",
      "Epoch: 33, Batch: 400, Loss: 0.0201\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i , data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
