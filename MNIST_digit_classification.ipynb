{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self,csv_file,transform=None,is_test=False):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)  #return the total number of samples in the dataset.\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_frame.iloc[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            image = item.values.reshape(28,28).astype(np.uint8)\n",
    "            label = None\n",
    "        else:\n",
    "            image = item[1:].values.reshape(28,28).astype(np.uint8)\n",
    "            label = item.iloc[0]\n",
    "\n",
    "        image = transforms.ToPILImage()(image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.RandomRotation(15),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5),)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomMNISTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomMNISTDataset(csv_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,transform\u001b[38;5;241m=\u001b[39mtransform,is_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mCustomMNISTDataset.__init__\u001b[1;34m(self, csv_file, transform, is_test)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,csv_file,transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,is_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_test \u001b[38;5;241m=\u001b[39m is_test\n",
      "File \u001b[1;32mc:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "train_dataset = CustomMNISTDataset(csv_file='train.csv',transform=transform,is_test=False)\n",
    "test_dataset = CustomMNISTDataset(csv_file='test.csv',transform=transform,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 42000 Test Size: 28000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Size: {len(train_dataset)} Test Size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000,  0.0745,  0.5059, -0.3255,\n",
       "           -0.4353,  0.1843, -0.7490, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.8588, -0.7647,  0.0745,  0.9922,  0.9922,  0.7020,\n",
       "            0.9294,  0.9922,  0.8118, -0.5765, -0.8824, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -0.3255,  0.9608,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922, -0.1843, -0.3333, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8980,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059,  0.3412,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,  0.4039,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922, -0.1451, -0.3490,\n",
       "            0.5608, -0.7725,  0.5686,  0.9922,  0.9922,  0.9922,  0.6627,\n",
       "           -0.4039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4353,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.1529, -0.6471, -1.0000,\n",
       "           -0.9137, -0.7725,  0.0039,  0.9765,  0.9922,  0.9922,  0.9922,\n",
       "            0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5216,  0.4980,\n",
       "            0.9922,  0.9922,  0.5843,  0.1529, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.3490,  0.9922,  0.9922,  0.9922,\n",
       "            0.2000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3490,  0.9922,\n",
       "            0.9922, -0.3020, -0.4745, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8039,  0.8824,  0.9922,\n",
       "            0.9922,  0.7569, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.9922,  0.3647,  0.9922,\n",
       "            0.9922, -0.7725, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3020,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6314,  0.9922,\n",
       "            0.9922,  0.8824, -0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4118,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.9922,\n",
       "            0.9922,  0.4588, -0.9451, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,  0.9922,\n",
       "            0.9922,  0.9922, -0.8667, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4980,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6235,\n",
       "            0.9922,  0.9922,  0.1765, -0.9922, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8902,  0.8196,  0.9922,\n",
       "            0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,  0.6392,\n",
       "            0.9922,  0.9216, -0.7569, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922, -0.7725, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.8353,  0.2627,  0.9922,\n",
       "            0.9922,  0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,  0.2784,\n",
       "            0.9922,  0.9922,  0.9922,  0.5686, -0.9059, -0.4824, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.6235,  0.6392,  0.9922,  0.9922,\n",
       "            0.9922, -0.3255, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.2627,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.5843, -0.4824, -0.6235,\n",
       "           -0.6235, -0.7333, -0.6784,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "            0.8745, -0.3255, -0.9137, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8824,\n",
       "            0.6157,  0.6627,  0.9922,  0.9922,  0.9922,  0.5216,  0.9922,\n",
       "            0.9922,  0.9922,  0.8275,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "           -0.4510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -0.5294, -0.3255,  0.9059,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,  0.9059,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.1059,  0.9922,  0.9922,  0.9922,\n",
       "            0.9922,  0.9922,  0.9922,  1.0000,  0.1451, -0.8510, -0.8824,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -0.8980,  0.4275,  0.9922,  0.1451,\n",
       "            0.9922,  1.0000,  0.9922, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -0.9373, -0.4039, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # num_workers=2 slows down the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for example_data, example_label in train_loader:\n",
    "    example_image = example_data[0]\n",
    "    print(f\"input size: {example_image.size()}\")\n",
    "    example_image_numpy = example_image.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(example_image_numpy)\n",
    "    plt.title(f\"Label: {example_label[0]}\")\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.cov1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.cov2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.cov3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        # Update the input size for fc1\n",
    "        self.fc1 = nn.Linear(128 * 26 * 26, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.cov1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.cov2(x)\n",
    "        x = self.cov3(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)   # Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, Loss: 1.9959\n",
      "Epoch: 1, Batch: 200, Loss: 0.6479\n",
      "Epoch: 1, Batch: 300, Loss: 0.3934\n",
      "Epoch: 1, Batch: 400, Loss: 0.3547\n",
      "Epoch: 1, Batch: 500, Loss: 0.3055\n",
      "Epoch: 1, Batch: 600, Loss: 0.2702\n",
      "Epoch: 2, Batch: 100, Loss: 0.3571\n",
      "Epoch: 2, Batch: 200, Loss: 0.2024\n",
      "Epoch: 2, Batch: 300, Loss: 0.1744\n",
      "Epoch: 2, Batch: 400, Loss: 0.1484\n",
      "Epoch: 2, Batch: 500, Loss: 0.1457\n",
      "Epoch: 2, Batch: 600, Loss: 0.1260\n",
      "Epoch: 3, Batch: 100, Loss: 0.1924\n",
      "Epoch: 3, Batch: 200, Loss: 0.1072\n",
      "Epoch: 3, Batch: 300, Loss: 0.1048\n",
      "Epoch: 3, Batch: 400, Loss: 0.0995\n",
      "Epoch: 3, Batch: 500, Loss: 0.1009\n",
      "Epoch: 3, Batch: 600, Loss: 0.0908\n",
      "Epoch: 4, Batch: 100, Loss: 0.1475\n",
      "Epoch: 4, Batch: 200, Loss: 0.0729\n",
      "Epoch: 4, Batch: 300, Loss: 0.0793\n",
      "Epoch: 4, Batch: 400, Loss: 0.0740\n",
      "Epoch: 4, Batch: 500, Loss: 0.0789\n",
      "Epoch: 4, Batch: 600, Loss: 0.0798\n",
      "Epoch: 5, Batch: 100, Loss: 0.1142\n",
      "Epoch: 5, Batch: 200, Loss: 0.0722\n",
      "Epoch: 5, Batch: 300, Loss: 0.0660\n",
      "Epoch: 5, Batch: 400, Loss: 0.0642\n",
      "Epoch: 5, Batch: 500, Loss: 0.0631\n",
      "Epoch: 5, Batch: 600, Loss: 0.0721\n",
      "Epoch: 6, Batch: 100, Loss: 0.0927\n",
      "Epoch: 6, Batch: 200, Loss: 0.0592\n",
      "Epoch: 6, Batch: 300, Loss: 0.0649\n",
      "Epoch: 6, Batch: 400, Loss: 0.0541\n",
      "Epoch: 6, Batch: 500, Loss: 0.0598\n",
      "Epoch: 6, Batch: 600, Loss: 0.0645\n",
      "Epoch: 7, Batch: 100, Loss: 0.0776\n",
      "Epoch: 7, Batch: 200, Loss: 0.0536\n",
      "Epoch: 7, Batch: 300, Loss: 0.0597\n",
      "Epoch: 7, Batch: 400, Loss: 0.0537\n",
      "Epoch: 7, Batch: 500, Loss: 0.0459\n",
      "Epoch: 7, Batch: 600, Loss: 0.0583\n",
      "Epoch: 8, Batch: 100, Loss: 0.0825\n",
      "Epoch: 8, Batch: 200, Loss: 0.0438\n",
      "Epoch: 8, Batch: 300, Loss: 0.0470\n",
      "Epoch: 8, Batch: 400, Loss: 0.0477\n",
      "Epoch: 8, Batch: 500, Loss: 0.0454\n",
      "Epoch: 8, Batch: 600, Loss: 0.0524\n",
      "Epoch: 9, Batch: 100, Loss: 0.0713\n",
      "Epoch: 9, Batch: 200, Loss: 0.0454\n",
      "Epoch: 9, Batch: 300, Loss: 0.0452\n",
      "Epoch: 9, Batch: 400, Loss: 0.0457\n",
      "Epoch: 9, Batch: 500, Loss: 0.0485\n",
      "Epoch: 9, Batch: 600, Loss: 0.0432\n",
      "Epoch: 10, Batch: 100, Loss: 0.0542\n",
      "Epoch: 10, Batch: 200, Loss: 0.0401\n",
      "Epoch: 10, Batch: 300, Loss: 0.0464\n",
      "Epoch: 10, Batch: 400, Loss: 0.0397\n",
      "Epoch: 10, Batch: 500, Loss: 0.0372\n",
      "Epoch: 10, Batch: 600, Loss: 0.0440\n",
      "Epoch: 11, Batch: 100, Loss: 0.0495\n",
      "Epoch: 11, Batch: 200, Loss: 0.0322\n",
      "Epoch: 11, Batch: 300, Loss: 0.0358\n",
      "Epoch: 11, Batch: 400, Loss: 0.0421\n",
      "Epoch: 11, Batch: 500, Loss: 0.0450\n",
      "Epoch: 11, Batch: 600, Loss: 0.0389\n",
      "Epoch: 12, Batch: 100, Loss: 0.0645\n",
      "Epoch: 12, Batch: 200, Loss: 0.0270\n",
      "Epoch: 12, Batch: 300, Loss: 0.0315\n",
      "Epoch: 12, Batch: 400, Loss: 0.0312\n",
      "Epoch: 12, Batch: 500, Loss: 0.0334\n",
      "Epoch: 12, Batch: 600, Loss: 0.0453\n",
      "Epoch: 13, Batch: 100, Loss: 0.0501\n",
      "Epoch: 13, Batch: 200, Loss: 0.0313\n",
      "Epoch: 13, Batch: 300, Loss: 0.0344\n",
      "Epoch: 13, Batch: 400, Loss: 0.0375\n",
      "Epoch: 13, Batch: 500, Loss: 0.0376\n",
      "Epoch: 13, Batch: 600, Loss: 0.0288\n",
      "Epoch: 14, Batch: 100, Loss: 0.0421\n",
      "Epoch: 14, Batch: 200, Loss: 0.0317\n",
      "Epoch: 14, Batch: 300, Loss: 0.0339\n",
      "Epoch: 14, Batch: 400, Loss: 0.0305\n",
      "Epoch: 14, Batch: 500, Loss: 0.0317\n",
      "Epoch: 14, Batch: 600, Loss: 0.0374\n",
      "Epoch: 15, Batch: 100, Loss: 0.0467\n",
      "Epoch: 15, Batch: 200, Loss: 0.0270\n",
      "Epoch: 15, Batch: 300, Loss: 0.0293\n",
      "Epoch: 15, Batch: 400, Loss: 0.0348\n",
      "Epoch: 15, Batch: 500, Loss: 0.0267\n",
      "Epoch: 15, Batch: 600, Loss: 0.0276\n",
      "Epoch: 16, Batch: 100, Loss: 0.0402\n",
      "Epoch: 16, Batch: 200, Loss: 0.0323\n",
      "Epoch: 16, Batch: 300, Loss: 0.0282\n",
      "Epoch: 16, Batch: 400, Loss: 0.0252\n",
      "Epoch: 16, Batch: 500, Loss: 0.0270\n",
      "Epoch: 16, Batch: 600, Loss: 0.0294\n",
      "Epoch: 17, Batch: 100, Loss: 0.0475\n",
      "Epoch: 17, Batch: 200, Loss: 0.0283\n",
      "Epoch: 17, Batch: 300, Loss: 0.0259\n",
      "Epoch: 17, Batch: 400, Loss: 0.0217\n",
      "Epoch: 17, Batch: 500, Loss: 0.0246\n",
      "Epoch: 17, Batch: 600, Loss: 0.0283\n",
      "Epoch: 18, Batch: 100, Loss: 0.0300\n",
      "Epoch: 18, Batch: 200, Loss: 0.0193\n",
      "Epoch: 18, Batch: 300, Loss: 0.0222\n",
      "Epoch: 18, Batch: 400, Loss: 0.0264\n",
      "Epoch: 18, Batch: 500, Loss: 0.0252\n",
      "Epoch: 18, Batch: 600, Loss: 0.0289\n",
      "Epoch: 19, Batch: 100, Loss: 0.0404\n",
      "Epoch: 19, Batch: 200, Loss: 0.0207\n",
      "Epoch: 19, Batch: 300, Loss: 0.0210\n",
      "Epoch: 19, Batch: 400, Loss: 0.0219\n",
      "Epoch: 19, Batch: 500, Loss: 0.0215\n",
      "Epoch: 19, Batch: 600, Loss: 0.0257\n",
      "Epoch: 20, Batch: 100, Loss: 0.0375\n",
      "Epoch: 20, Batch: 200, Loss: 0.0221\n",
      "Epoch: 20, Batch: 300, Loss: 0.0233\n",
      "Epoch: 20, Batch: 400, Loss: 0.0222\n",
      "Epoch: 20, Batch: 500, Loss: 0.0257\n",
      "Epoch: 20, Batch: 600, Loss: 0.0222\n",
      "Epoch: 21, Batch: 100, Loss: 0.0337\n",
      "Epoch: 21, Batch: 200, Loss: 0.0151\n",
      "Epoch: 21, Batch: 300, Loss: 0.0252\n",
      "Epoch: 21, Batch: 400, Loss: 0.0210\n",
      "Epoch: 21, Batch: 500, Loss: 0.0193\n",
      "Epoch: 21, Batch: 600, Loss: 0.0276\n",
      "Epoch: 22, Batch: 100, Loss: 0.0304\n",
      "Epoch: 22, Batch: 200, Loss: 0.0187\n",
      "Epoch: 22, Batch: 300, Loss: 0.0183\n",
      "Epoch: 22, Batch: 400, Loss: 0.0195\n",
      "Epoch: 22, Batch: 500, Loss: 0.0201\n",
      "Epoch: 22, Batch: 600, Loss: 0.0151\n",
      "Epoch: 23, Batch: 100, Loss: 0.0253\n",
      "Epoch: 23, Batch: 200, Loss: 0.0203\n",
      "Epoch: 23, Batch: 300, Loss: 0.0184\n",
      "Epoch: 23, Batch: 400, Loss: 0.0199\n",
      "Epoch: 23, Batch: 500, Loss: 0.0202\n",
      "Epoch: 23, Batch: 600, Loss: 0.0193\n",
      "Epoch: 24, Batch: 100, Loss: 0.0288\n",
      "Epoch: 24, Batch: 200, Loss: 0.0190\n",
      "Epoch: 24, Batch: 300, Loss: 0.0156\n",
      "Epoch: 24, Batch: 400, Loss: 0.0228\n",
      "Epoch: 24, Batch: 500, Loss: 0.0166\n",
      "Epoch: 24, Batch: 600, Loss: 0.0237\n",
      "Epoch: 25, Batch: 100, Loss: 0.0240\n",
      "Epoch: 25, Batch: 200, Loss: 0.0150\n",
      "Epoch: 25, Batch: 300, Loss: 0.0158\n",
      "Epoch: 25, Batch: 400, Loss: 0.0180\n",
      "Epoch: 25, Batch: 500, Loss: 0.0170\n",
      "Epoch: 25, Batch: 600, Loss: 0.0144\n",
      "Epoch: 26, Batch: 100, Loss: 0.0257\n",
      "Epoch: 26, Batch: 200, Loss: 0.0140\n",
      "Epoch: 26, Batch: 300, Loss: 0.0148\n",
      "Epoch: 26, Batch: 400, Loss: 0.0158\n",
      "Epoch: 26, Batch: 500, Loss: 0.0192\n",
      "Epoch: 26, Batch: 600, Loss: 0.0169\n",
      "Epoch: 27, Batch: 100, Loss: 0.0202\n",
      "Epoch: 27, Batch: 200, Loss: 0.0141\n",
      "Epoch: 27, Batch: 300, Loss: 0.0156\n",
      "Epoch: 27, Batch: 400, Loss: 0.0170\n",
      "Epoch: 27, Batch: 500, Loss: 0.0161\n",
      "Epoch: 27, Batch: 600, Loss: 0.0204\n",
      "Epoch: 28, Batch: 100, Loss: 0.0227\n",
      "Epoch: 28, Batch: 200, Loss: 0.0169\n",
      "Epoch: 28, Batch: 300, Loss: 0.0147\n",
      "Epoch: 28, Batch: 400, Loss: 0.0136\n",
      "Epoch: 28, Batch: 500, Loss: 0.0143\n",
      "Epoch: 28, Batch: 600, Loss: 0.0112\n",
      "Epoch: 29, Batch: 100, Loss: 0.0205\n",
      "Epoch: 29, Batch: 200, Loss: 0.0159\n",
      "Epoch: 29, Batch: 300, Loss: 0.0108\n",
      "Epoch: 29, Batch: 400, Loss: 0.0132\n",
      "Epoch: 29, Batch: 500, Loss: 0.0125\n",
      "Epoch: 29, Batch: 600, Loss: 0.0122\n",
      "Epoch: 30, Batch: 100, Loss: 0.0176\n",
      "Epoch: 30, Batch: 200, Loss: 0.0126\n",
      "Epoch: 30, Batch: 300, Loss: 0.0124\n",
      "Epoch: 30, Batch: 400, Loss: 0.0165\n",
      "Epoch: 30, Batch: 500, Loss: 0.0126\n",
      "Epoch: 30, Batch: 600, Loss: 0.0138\n",
      "Epoch: 31, Batch: 100, Loss: 0.0243\n",
      "Epoch: 31, Batch: 200, Loss: 0.0109\n",
      "Epoch: 31, Batch: 300, Loss: 0.0154\n",
      "Epoch: 31, Batch: 400, Loss: 0.0152\n",
      "Epoch: 31, Batch: 500, Loss: 0.0138\n",
      "Epoch: 31, Batch: 600, Loss: 0.0144\n",
      "Epoch: 32, Batch: 100, Loss: 0.0206\n",
      "Epoch: 32, Batch: 200, Loss: 0.0144\n",
      "Epoch: 32, Batch: 300, Loss: 0.0107\n",
      "Epoch: 32, Batch: 400, Loss: 0.0117\n",
      "Epoch: 32, Batch: 500, Loss: 0.0121\n",
      "Epoch: 32, Batch: 600, Loss: 0.0113\n",
      "Epoch: 33, Batch: 100, Loss: 0.0220\n",
      "Epoch: 33, Batch: 200, Loss: 0.0110\n",
      "Epoch: 33, Batch: 300, Loss: 0.0137\n",
      "Epoch: 33, Batch: 400, Loss: 0.0154\n",
      "Epoch: 33, Batch: 500, Loss: 0.0121\n",
      "Epoch: 33, Batch: 600, Loss: 0.0088\n",
      "Epoch: 34, Batch: 100, Loss: 0.0231\n",
      "Epoch: 34, Batch: 200, Loss: 0.0131\n",
      "Epoch: 34, Batch: 300, Loss: 0.0129\n",
      "Epoch: 34, Batch: 400, Loss: 0.0110\n",
      "Epoch: 34, Batch: 500, Loss: 0.0142\n",
      "Epoch: 34, Batch: 600, Loss: 0.0139\n",
      "Epoch: 35, Batch: 100, Loss: 0.0182\n",
      "Epoch: 35, Batch: 200, Loss: 0.0126\n",
      "Epoch: 35, Batch: 300, Loss: 0.0100\n",
      "Epoch: 35, Batch: 400, Loss: 0.0140\n",
      "Epoch: 35, Batch: 500, Loss: 0.0092\n",
      "Epoch: 35, Batch: 600, Loss: 0.0102\n",
      "Epoch: 36, Batch: 100, Loss: 0.0133\n",
      "Epoch: 36, Batch: 200, Loss: 0.0087\n",
      "Epoch: 36, Batch: 300, Loss: 0.0093\n",
      "Epoch: 36, Batch: 400, Loss: 0.0136\n",
      "Epoch: 36, Batch: 500, Loss: 0.0133\n",
      "Epoch: 36, Batch: 600, Loss: 0.0102\n",
      "Epoch: 37, Batch: 100, Loss: 0.0155\n",
      "Epoch: 37, Batch: 200, Loss: 0.0094\n",
      "Epoch: 37, Batch: 300, Loss: 0.0100\n",
      "Epoch: 37, Batch: 400, Loss: 0.0105\n",
      "Epoch: 37, Batch: 500, Loss: 0.0108\n",
      "Epoch: 37, Batch: 600, Loss: 0.0134\n",
      "Epoch: 38, Batch: 100, Loss: 0.0201\n",
      "Epoch: 38, Batch: 200, Loss: 0.0120\n",
      "Epoch: 38, Batch: 300, Loss: 0.0101\n",
      "Epoch: 38, Batch: 400, Loss: 0.0112\n",
      "Epoch: 38, Batch: 500, Loss: 0.0097\n",
      "Epoch: 38, Batch: 600, Loss: 0.0071\n",
      "Epoch: 39, Batch: 100, Loss: 0.0172\n",
      "Epoch: 39, Batch: 200, Loss: 0.0094\n",
      "Epoch: 39, Batch: 300, Loss: 0.0120\n",
      "Epoch: 39, Batch: 400, Loss: 0.0102\n",
      "Epoch: 39, Batch: 500, Loss: 0.0107\n",
      "Epoch: 39, Batch: 600, Loss: 0.0076\n",
      "Epoch: 40, Batch: 100, Loss: 0.0111\n",
      "Epoch: 40, Batch: 200, Loss: 0.0104\n",
      "Epoch: 40, Batch: 300, Loss: 0.0092\n",
      "Epoch: 40, Batch: 400, Loss: 0.0086\n",
      "Epoch: 40, Batch: 500, Loss: 0.0094\n",
      "Epoch: 40, Batch: 600, Loss: 0.0123\n",
      "Epoch: 41, Batch: 100, Loss: 0.0135\n",
      "Epoch: 41, Batch: 200, Loss: 0.0055\n",
      "Epoch: 41, Batch: 300, Loss: 0.0084\n",
      "Epoch: 41, Batch: 400, Loss: 0.0116\n",
      "Epoch: 41, Batch: 500, Loss: 0.0104\n",
      "Epoch: 41, Batch: 600, Loss: 0.0081\n",
      "Epoch: 42, Batch: 100, Loss: 0.0128\n",
      "Epoch: 42, Batch: 200, Loss: 0.0083\n",
      "Epoch: 42, Batch: 300, Loss: 0.0092\n",
      "Epoch: 42, Batch: 400, Loss: 0.0103\n",
      "Epoch: 42, Batch: 500, Loss: 0.0101\n",
      "Epoch: 42, Batch: 600, Loss: 0.0119\n",
      "Epoch: 43, Batch: 100, Loss: 0.0172\n",
      "Epoch: 43, Batch: 200, Loss: 0.0107\n",
      "Epoch: 43, Batch: 300, Loss: 0.0072\n",
      "Epoch: 43, Batch: 400, Loss: 0.0116\n",
      "Epoch: 43, Batch: 500, Loss: 0.0076\n",
      "Epoch: 43, Batch: 600, Loss: 0.0090\n",
      "Epoch: 44, Batch: 100, Loss: 0.0112\n",
      "Epoch: 44, Batch: 200, Loss: 0.0114\n",
      "Epoch: 44, Batch: 300, Loss: 0.0075\n",
      "Epoch: 44, Batch: 400, Loss: 0.0076\n",
      "Epoch: 44, Batch: 500, Loss: 0.0065\n",
      "Epoch: 44, Batch: 600, Loss: 0.0141\n",
      "Epoch: 45, Batch: 100, Loss: 0.0129\n",
      "Epoch: 45, Batch: 200, Loss: 0.0085\n",
      "Epoch: 45, Batch: 300, Loss: 0.0091\n",
      "Epoch: 45, Batch: 400, Loss: 0.0093\n",
      "Epoch: 45, Batch: 500, Loss: 0.0071\n",
      "Epoch: 45, Batch: 600, Loss: 0.0108\n",
      "Epoch: 46, Batch: 100, Loss: 0.0148\n",
      "Epoch: 46, Batch: 200, Loss: 0.0082\n",
      "Epoch: 46, Batch: 300, Loss: 0.0068\n",
      "Epoch: 46, Batch: 400, Loss: 0.0068\n",
      "Epoch: 46, Batch: 500, Loss: 0.0087\n",
      "Epoch: 46, Batch: 600, Loss: 0.0093\n",
      "Epoch: 47, Batch: 100, Loss: 0.0099\n",
      "Epoch: 47, Batch: 200, Loss: 0.0084\n",
      "Epoch: 47, Batch: 300, Loss: 0.0081\n",
      "Epoch: 47, Batch: 400, Loss: 0.0098\n",
      "Epoch: 47, Batch: 500, Loss: 0.0117\n",
      "Epoch: 47, Batch: 600, Loss: 0.0080\n",
      "Epoch: 48, Batch: 100, Loss: 0.0128\n",
      "Epoch: 48, Batch: 200, Loss: 0.0096\n",
      "Epoch: 48, Batch: 300, Loss: 0.0054\n",
      "Epoch: 48, Batch: 400, Loss: 0.0093\n",
      "Epoch: 48, Batch: 500, Loss: 0.0095\n",
      "Epoch: 48, Batch: 600, Loss: 0.0109\n",
      "Epoch: 49, Batch: 100, Loss: 0.0126\n",
      "Epoch: 49, Batch: 200, Loss: 0.0070\n",
      "Epoch: 49, Batch: 300, Loss: 0.0079\n",
      "Epoch: 49, Batch: 400, Loss: 0.0080\n",
      "Epoch: 49, Batch: 500, Loss: 0.0075\n",
      "Epoch: 49, Batch: 600, Loss: 0.0060\n",
      "Epoch: 50, Batch: 100, Loss: 0.0098\n",
      "Epoch: 50, Batch: 200, Loss: 0.0064\n",
      "Epoch: 50, Batch: 300, Loss: 0.0064\n",
      "Epoch: 50, Batch: 400, Loss: 0.0044\n",
      "Epoch: 50, Batch: 500, Loss: 0.0101\n",
      "Epoch: 50, Batch: 600, Loss: 0.0086\n",
      "Epoch: 51, Batch: 100, Loss: 0.0098\n",
      "Epoch: 51, Batch: 200, Loss: 0.0050\n",
      "Epoch: 51, Batch: 300, Loss: 0.0086\n",
      "Epoch: 51, Batch: 400, Loss: 0.0078\n",
      "Epoch: 51, Batch: 500, Loss: 0.0088\n",
      "Epoch: 51, Batch: 600, Loss: 0.0058\n",
      "Epoch: 52, Batch: 100, Loss: 0.0101\n",
      "Epoch: 52, Batch: 200, Loss: 0.0050\n",
      "Epoch: 52, Batch: 300, Loss: 0.0056\n",
      "Epoch: 52, Batch: 400, Loss: 0.0076\n",
      "Epoch: 52, Batch: 500, Loss: 0.0089\n",
      "Epoch: 52, Batch: 600, Loss: 0.0089\n",
      "Epoch: 53, Batch: 100, Loss: 0.0081\n",
      "Epoch: 53, Batch: 200, Loss: 0.0054\n",
      "Epoch: 53, Batch: 300, Loss: 0.0067\n",
      "Epoch: 53, Batch: 400, Loss: 0.0054\n",
      "Epoch: 53, Batch: 500, Loss: 0.0053\n",
      "Epoch: 53, Batch: 600, Loss: 0.0061\n",
      "Epoch: 54, Batch: 100, Loss: 0.0130\n",
      "Epoch: 54, Batch: 200, Loss: 0.0053\n",
      "Epoch: 54, Batch: 300, Loss: 0.0084\n",
      "Epoch: 54, Batch: 400, Loss: 0.0058\n",
      "Epoch: 54, Batch: 500, Loss: 0.0065\n",
      "Epoch: 54, Batch: 600, Loss: 0.0079\n",
      "Epoch: 55, Batch: 100, Loss: 0.0097\n",
      "Epoch: 55, Batch: 200, Loss: 0.0072\n",
      "Epoch: 55, Batch: 300, Loss: 0.0062\n",
      "Epoch: 55, Batch: 400, Loss: 0.0060\n",
      "Epoch: 55, Batch: 500, Loss: 0.0048\n",
      "Epoch: 55, Batch: 600, Loss: 0.0069\n",
      "Epoch: 56, Batch: 100, Loss: 0.0105\n",
      "Epoch: 56, Batch: 200, Loss: 0.0054\n",
      "Epoch: 56, Batch: 300, Loss: 0.0070\n",
      "Epoch: 56, Batch: 400, Loss: 0.0063\n",
      "Epoch: 56, Batch: 500, Loss: 0.0072\n",
      "Epoch: 56, Batch: 600, Loss: 0.0102\n",
      "Epoch: 57, Batch: 100, Loss: 0.0122\n",
      "Epoch: 57, Batch: 200, Loss: 0.0051\n",
      "Epoch: 57, Batch: 300, Loss: 0.0076\n",
      "Epoch: 57, Batch: 400, Loss: 0.0088\n",
      "Epoch: 57, Batch: 500, Loss: 0.0078\n",
      "Epoch: 57, Batch: 600, Loss: 0.0055\n",
      "Epoch: 58, Batch: 100, Loss: 0.0086\n",
      "Epoch: 58, Batch: 200, Loss: 0.0078\n",
      "Epoch: 58, Batch: 300, Loss: 0.0048\n",
      "Epoch: 58, Batch: 400, Loss: 0.0054\n",
      "Epoch: 58, Batch: 500, Loss: 0.0069\n",
      "Epoch: 58, Batch: 600, Loss: 0.0058\n",
      "Epoch: 59, Batch: 100, Loss: 0.0058\n",
      "Epoch: 59, Batch: 200, Loss: 0.0050\n",
      "Epoch: 59, Batch: 300, Loss: 0.0064\n",
      "Epoch: 59, Batch: 400, Loss: 0.0045\n",
      "Epoch: 59, Batch: 500, Loss: 0.0040\n",
      "Epoch: 59, Batch: 600, Loss: 0.0038\n",
      "Epoch: 60, Batch: 100, Loss: 0.0082\n",
      "Epoch: 60, Batch: 200, Loss: 0.0045\n",
      "Epoch: 60, Batch: 300, Loss: 0.0030\n",
      "Epoch: 60, Batch: 400, Loss: 0.0054\n",
      "Epoch: 60, Batch: 500, Loss: 0.0056\n",
      "Epoch: 60, Batch: 600, Loss: 0.0076\n",
      "Epoch: 61, Batch: 100, Loss: 0.0070\n",
      "Epoch: 61, Batch: 200, Loss: 0.0078\n",
      "Epoch: 61, Batch: 300, Loss: 0.0078\n",
      "Epoch: 61, Batch: 400, Loss: 0.0045\n",
      "Epoch: 61, Batch: 500, Loss: 0.0044\n",
      "Epoch: 61, Batch: 600, Loss: 0.0077\n",
      "Epoch: 62, Batch: 100, Loss: 0.0083\n",
      "Epoch: 62, Batch: 200, Loss: 0.0037\n",
      "Epoch: 62, Batch: 300, Loss: 0.0042\n",
      "Epoch: 62, Batch: 400, Loss: 0.0064\n",
      "Epoch: 62, Batch: 500, Loss: 0.0049\n",
      "Epoch: 62, Batch: 600, Loss: 0.0059\n",
      "Epoch: 63, Batch: 100, Loss: 0.0074\n",
      "Epoch: 63, Batch: 200, Loss: 0.0043\n",
      "Epoch: 63, Batch: 300, Loss: 0.0070\n",
      "Epoch: 63, Batch: 400, Loss: 0.0060\n",
      "Epoch: 63, Batch: 500, Loss: 0.0050\n",
      "Epoch: 63, Batch: 600, Loss: 0.0060\n",
      "Epoch: 64, Batch: 100, Loss: 0.0061\n",
      "Epoch: 64, Batch: 200, Loss: 0.0052\n",
      "Epoch: 64, Batch: 300, Loss: 0.0050\n",
      "Epoch: 64, Batch: 400, Loss: 0.0062\n",
      "Epoch: 64, Batch: 500, Loss: 0.0058\n",
      "Epoch: 64, Batch: 600, Loss: 0.0048\n",
      "Epoch: 65, Batch: 100, Loss: 0.0078\n",
      "Epoch: 65, Batch: 200, Loss: 0.0051\n",
      "Epoch: 65, Batch: 300, Loss: 0.0071\n",
      "Epoch: 65, Batch: 400, Loss: 0.0044\n",
      "Epoch: 65, Batch: 500, Loss: 0.0053\n",
      "Epoch: 65, Batch: 600, Loss: 0.0051\n",
      "Epoch: 66, Batch: 100, Loss: 0.0085\n",
      "Epoch: 66, Batch: 200, Loss: 0.0051\n",
      "Epoch: 66, Batch: 300, Loss: 0.0074\n",
      "Epoch: 66, Batch: 400, Loss: 0.0033\n",
      "Epoch: 66, Batch: 500, Loss: 0.0074\n",
      "Epoch: 66, Batch: 600, Loss: 0.0039\n",
      "Epoch: 67, Batch: 100, Loss: 0.0076\n",
      "Epoch: 67, Batch: 200, Loss: 0.0042\n",
      "Epoch: 67, Batch: 300, Loss: 0.0036\n",
      "Epoch: 67, Batch: 400, Loss: 0.0074\n",
      "Epoch: 67, Batch: 500, Loss: 0.0047\n",
      "Epoch: 67, Batch: 600, Loss: 0.0047\n",
      "Epoch: 68, Batch: 100, Loss: 0.0045\n",
      "Epoch: 68, Batch: 200, Loss: 0.0017\n",
      "Epoch: 68, Batch: 300, Loss: 0.0040\n",
      "Epoch: 68, Batch: 400, Loss: 0.0023\n",
      "Epoch: 68, Batch: 500, Loss: 0.0039\n",
      "Epoch: 68, Batch: 600, Loss: 0.0094\n",
      "Epoch: 69, Batch: 100, Loss: 0.0060\n",
      "Epoch: 69, Batch: 200, Loss: 0.0066\n",
      "Epoch: 69, Batch: 300, Loss: 0.0046\n",
      "Epoch: 69, Batch: 400, Loss: 0.0052\n",
      "Epoch: 69, Batch: 500, Loss: 0.0048\n",
      "Epoch: 69, Batch: 600, Loss: 0.0080\n",
      "Epoch: 70, Batch: 100, Loss: 0.0108\n",
      "Epoch: 70, Batch: 200, Loss: 0.0044\n",
      "Epoch: 70, Batch: 300, Loss: 0.0034\n",
      "Epoch: 70, Batch: 400, Loss: 0.0024\n",
      "Epoch: 70, Batch: 500, Loss: 0.0034\n",
      "Epoch: 70, Batch: 600, Loss: 0.0047\n",
      "Epoch: 71, Batch: 100, Loss: 0.0084\n",
      "Epoch: 71, Batch: 200, Loss: 0.0043\n",
      "Epoch: 71, Batch: 300, Loss: 0.0048\n",
      "Epoch: 71, Batch: 400, Loss: 0.0038\n",
      "Epoch: 71, Batch: 500, Loss: 0.0061\n",
      "Epoch: 71, Batch: 600, Loss: 0.0037\n",
      "Epoch: 72, Batch: 100, Loss: 0.0140\n",
      "Epoch: 72, Batch: 200, Loss: 0.0058\n",
      "Epoch: 72, Batch: 300, Loss: 0.0036\n",
      "Epoch: 72, Batch: 400, Loss: 0.0063\n",
      "Epoch: 72, Batch: 500, Loss: 0.0043\n",
      "Epoch: 72, Batch: 600, Loss: 0.0048\n",
      "Epoch: 73, Batch: 100, Loss: 0.0090\n",
      "Epoch: 73, Batch: 200, Loss: 0.0050\n",
      "Epoch: 73, Batch: 300, Loss: 0.0051\n",
      "Epoch: 73, Batch: 400, Loss: 0.0070\n",
      "Epoch: 73, Batch: 500, Loss: 0.0049\n",
      "Epoch: 73, Batch: 600, Loss: 0.0049\n",
      "Epoch: 74, Batch: 100, Loss: 0.0099\n",
      "Epoch: 74, Batch: 200, Loss: 0.0053\n",
      "Epoch: 74, Batch: 300, Loss: 0.0051\n",
      "Epoch: 74, Batch: 400, Loss: 0.0052\n",
      "Epoch: 74, Batch: 500, Loss: 0.0070\n",
      "Epoch: 74, Batch: 600, Loss: 0.0067\n",
      "Epoch: 75, Batch: 100, Loss: 0.0066\n",
      "Epoch: 75, Batch: 200, Loss: 0.0040\n",
      "Epoch: 75, Batch: 300, Loss: 0.0062\n",
      "Epoch: 75, Batch: 400, Loss: 0.0038\n",
      "Epoch: 75, Batch: 500, Loss: 0.0036\n",
      "Epoch: 75, Batch: 600, Loss: 0.0053\n",
      "Epoch: 76, Batch: 100, Loss: 0.0048\n",
      "Epoch: 76, Batch: 200, Loss: 0.0039\n",
      "Epoch: 76, Batch: 300, Loss: 0.0032\n",
      "Epoch: 76, Batch: 400, Loss: 0.0058\n",
      "Epoch: 76, Batch: 500, Loss: 0.0045\n",
      "Epoch: 76, Batch: 600, Loss: 0.0025\n",
      "Epoch: 77, Batch: 100, Loss: 0.0126\n",
      "Epoch: 77, Batch: 200, Loss: 0.0039\n",
      "Epoch: 77, Batch: 300, Loss: 0.0063\n",
      "Epoch: 77, Batch: 400, Loss: 0.0055\n",
      "Epoch: 77, Batch: 500, Loss: 0.0036\n",
      "Epoch: 77, Batch: 600, Loss: 0.0051\n",
      "Epoch: 78, Batch: 100, Loss: 0.0040\n",
      "Epoch: 78, Batch: 200, Loss: 0.0031\n",
      "Epoch: 78, Batch: 300, Loss: 0.0068\n",
      "Epoch: 78, Batch: 400, Loss: 0.0031\n",
      "Epoch: 78, Batch: 500, Loss: 0.0035\n",
      "Epoch: 78, Batch: 600, Loss: 0.0070\n",
      "Epoch: 79, Batch: 100, Loss: 0.0052\n",
      "Epoch: 79, Batch: 200, Loss: 0.0025\n",
      "Epoch: 79, Batch: 300, Loss: 0.0026\n",
      "Epoch: 79, Batch: 400, Loss: 0.0026\n",
      "Epoch: 79, Batch: 500, Loss: 0.0049\n",
      "Epoch: 79, Batch: 600, Loss: 0.0050\n",
      "Epoch: 80, Batch: 100, Loss: 0.0092\n",
      "Epoch: 80, Batch: 200, Loss: 0.0047\n",
      "Epoch: 80, Batch: 300, Loss: 0.0057\n",
      "Epoch: 80, Batch: 400, Loss: 0.0030\n",
      "Epoch: 80, Batch: 500, Loss: 0.0045\n",
      "Epoch: 80, Batch: 600, Loss: 0.0036\n",
      "Epoch: 81, Batch: 100, Loss: 0.0057\n",
      "Epoch: 81, Batch: 200, Loss: 0.0037\n",
      "Epoch: 81, Batch: 300, Loss: 0.0041\n",
      "Epoch: 81, Batch: 400, Loss: 0.0043\n",
      "Epoch: 81, Batch: 500, Loss: 0.0039\n",
      "Epoch: 81, Batch: 600, Loss: 0.0049\n",
      "Epoch: 82, Batch: 100, Loss: 0.0051\n",
      "Epoch: 82, Batch: 200, Loss: 0.0023\n",
      "Epoch: 82, Batch: 300, Loss: 0.0032\n",
      "Epoch: 82, Batch: 400, Loss: 0.0052\n",
      "Epoch: 82, Batch: 500, Loss: 0.0034\n",
      "Epoch: 82, Batch: 600, Loss: 0.0030\n",
      "Epoch: 83, Batch: 100, Loss: 0.0043\n",
      "Epoch: 83, Batch: 200, Loss: 0.0043\n",
      "Epoch: 83, Batch: 300, Loss: 0.0043\n",
      "Epoch: 83, Batch: 400, Loss: 0.0032\n",
      "Epoch: 83, Batch: 500, Loss: 0.0055\n",
      "Epoch: 83, Batch: 600, Loss: 0.0034\n",
      "Epoch: 84, Batch: 100, Loss: 0.0050\n",
      "Epoch: 84, Batch: 200, Loss: 0.0025\n",
      "Epoch: 84, Batch: 300, Loss: 0.0022\n",
      "Epoch: 84, Batch: 400, Loss: 0.0026\n",
      "Epoch: 84, Batch: 500, Loss: 0.0035\n",
      "Epoch: 84, Batch: 600, Loss: 0.0030\n",
      "Epoch: 85, Batch: 100, Loss: 0.0048\n",
      "Epoch: 85, Batch: 200, Loss: 0.0022\n",
      "Epoch: 85, Batch: 300, Loss: 0.0028\n",
      "Epoch: 85, Batch: 400, Loss: 0.0035\n",
      "Epoch: 85, Batch: 500, Loss: 0.0045\n",
      "Epoch: 85, Batch: 600, Loss: 0.0031\n",
      "Epoch: 86, Batch: 100, Loss: 0.0041\n",
      "Epoch: 86, Batch: 200, Loss: 0.0019\n",
      "Epoch: 86, Batch: 300, Loss: 0.0024\n",
      "Epoch: 86, Batch: 400, Loss: 0.0014\n",
      "Epoch: 86, Batch: 500, Loss: 0.0029\n",
      "Epoch: 86, Batch: 600, Loss: 0.0045\n",
      "Epoch: 87, Batch: 100, Loss: 0.0049\n",
      "Epoch: 87, Batch: 200, Loss: 0.0015\n",
      "Epoch: 87, Batch: 300, Loss: 0.0028\n",
      "Epoch: 87, Batch: 400, Loss: 0.0023\n",
      "Epoch: 87, Batch: 500, Loss: 0.0034\n",
      "Epoch: 87, Batch: 600, Loss: 0.0034\n",
      "Epoch: 88, Batch: 100, Loss: 0.0050\n",
      "Epoch: 88, Batch: 200, Loss: 0.0017\n",
      "Epoch: 88, Batch: 300, Loss: 0.0039\n",
      "Epoch: 88, Batch: 400, Loss: 0.0041\n",
      "Epoch: 88, Batch: 500, Loss: 0.0016\n",
      "Epoch: 88, Batch: 600, Loss: 0.0044\n",
      "Epoch: 89, Batch: 100, Loss: 0.0049\n",
      "Epoch: 89, Batch: 200, Loss: 0.0036\n",
      "Epoch: 89, Batch: 300, Loss: 0.0037\n",
      "Epoch: 89, Batch: 400, Loss: 0.0018\n",
      "Epoch: 89, Batch: 500, Loss: 0.0029\n",
      "Epoch: 89, Batch: 600, Loss: 0.0026\n",
      "Epoch: 90, Batch: 100, Loss: 0.0040\n",
      "Epoch: 90, Batch: 200, Loss: 0.0036\n",
      "Epoch: 90, Batch: 300, Loss: 0.0040\n",
      "Epoch: 90, Batch: 400, Loss: 0.0032\n",
      "Epoch: 90, Batch: 500, Loss: 0.0035\n",
      "Epoch: 90, Batch: 600, Loss: 0.0034\n",
      "Epoch: 91, Batch: 100, Loss: 0.0049\n",
      "Epoch: 91, Batch: 200, Loss: 0.0049\n",
      "Epoch: 91, Batch: 300, Loss: 0.0031\n",
      "Epoch: 91, Batch: 400, Loss: 0.0033\n",
      "Epoch: 91, Batch: 500, Loss: 0.0041\n",
      "Epoch: 91, Batch: 600, Loss: 0.0019\n",
      "Epoch: 92, Batch: 100, Loss: 0.0072\n",
      "Epoch: 92, Batch: 200, Loss: 0.0050\n",
      "Epoch: 92, Batch: 300, Loss: 0.0030\n",
      "Epoch: 92, Batch: 400, Loss: 0.0052\n",
      "Epoch: 92, Batch: 500, Loss: 0.0032\n",
      "Epoch: 92, Batch: 600, Loss: 0.0030\n",
      "Epoch: 93, Batch: 100, Loss: 0.0028\n",
      "Epoch: 93, Batch: 200, Loss: 0.0023\n",
      "Epoch: 93, Batch: 300, Loss: 0.0031\n",
      "Epoch: 93, Batch: 400, Loss: 0.0038\n",
      "Epoch: 93, Batch: 500, Loss: 0.0023\n",
      "Epoch: 93, Batch: 600, Loss: 0.0018\n",
      "Epoch: 94, Batch: 100, Loss: 0.0058\n",
      "Epoch: 94, Batch: 200, Loss: 0.0029\n",
      "Epoch: 94, Batch: 300, Loss: 0.0055\n",
      "Epoch: 94, Batch: 400, Loss: 0.0034\n",
      "Epoch: 94, Batch: 500, Loss: 0.0058\n",
      "Epoch: 94, Batch: 600, Loss: 0.0026\n",
      "Epoch: 95, Batch: 100, Loss: 0.0047\n",
      "Epoch: 95, Batch: 200, Loss: 0.0037\n",
      "Epoch: 95, Batch: 300, Loss: 0.0032\n",
      "Epoch: 95, Batch: 400, Loss: 0.0032\n",
      "Epoch: 95, Batch: 500, Loss: 0.0031\n",
      "Epoch: 95, Batch: 600, Loss: 0.0024\n",
      "Epoch: 96, Batch: 100, Loss: 0.0042\n",
      "Epoch: 96, Batch: 200, Loss: 0.0033\n",
      "Epoch: 96, Batch: 300, Loss: 0.0018\n",
      "Epoch: 96, Batch: 400, Loss: 0.0014\n",
      "Epoch: 96, Batch: 500, Loss: 0.0014\n",
      "Epoch: 96, Batch: 600, Loss: 0.0024\n",
      "Epoch: 97, Batch: 100, Loss: 0.0075\n",
      "Epoch: 97, Batch: 200, Loss: 0.0020\n",
      "Epoch: 97, Batch: 300, Loss: 0.0019\n",
      "Epoch: 97, Batch: 400, Loss: 0.0015\n",
      "Epoch: 97, Batch: 500, Loss: 0.0032\n",
      "Epoch: 97, Batch: 600, Loss: 0.0023\n",
      "Epoch: 98, Batch: 100, Loss: 0.0054\n",
      "Epoch: 98, Batch: 200, Loss: 0.0030\n",
      "Epoch: 98, Batch: 300, Loss: 0.0031\n",
      "Epoch: 98, Batch: 400, Loss: 0.0027\n",
      "Epoch: 98, Batch: 500, Loss: 0.0043\n",
      "Epoch: 98, Batch: 600, Loss: 0.0023\n",
      "Epoch: 99, Batch: 100, Loss: 0.0036\n",
      "Epoch: 99, Batch: 200, Loss: 0.0029\n",
      "Epoch: 99, Batch: 300, Loss: 0.0032\n",
      "Epoch: 99, Batch: 400, Loss: 0.0023\n",
      "Epoch: 99, Batch: 500, Loss: 0.0019\n",
      "Epoch: 99, Batch: 600, Loss: 0.0027\n",
      "Epoch: 100, Batch: 100, Loss: 0.0041\n",
      "Epoch: 100, Batch: 200, Loss: 0.0025\n",
      "Epoch: 100, Batch: 300, Loss: 0.0019\n",
      "Epoch: 100, Batch: 400, Loss: 0.0028\n",
      "Epoch: 100, Batch: 500, Loss: 0.0021\n",
      "Epoch: 100, Batch: 600, Loss: 0.0044\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i , data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (cov1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cov2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cov3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=86528, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (fc3): Linear(in_features=20, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ImageId': np.arange(1, len(predictions) + 1),\n",
    "    'Label': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
